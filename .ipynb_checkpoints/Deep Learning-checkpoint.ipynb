{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "This notebook is responsible for implementing a recurrent neural network using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"localhost\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe-ize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql as pms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great meeting with @Cabinet at the @WhiteHouse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking forward to 3:30 P.M. meeting today at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lowest rated Oscars in HISTORY. Problem is, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>JOBS, JOBS, JOBS! #MAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The U.S. is acting swiftly on Intellectual Pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message\n",
       "0   1  Great meeting with @Cabinet at the @WhiteHouse...\n",
       "1   2  Looking forward to 3:30 P.M. meeting today at ...\n",
       "2   3  Lowest rated Oscars in HISTORY. Problem is, we...\n",
       "3   4                            JOBS, JOBS, JOBS! #MAGA\n",
       "4   5  The U.S. is acting swiftly on Intellectual Pro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    con = pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name)\n",
    "    df = pd.read_sql(\"\"\"SELECT * FROM search_tweets\"\"\", con)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique space-separated character orderings: 12837\n",
      "Tweets: 2889\n",
      "Average sentences per tweet: 2.7608168916580134\n"
     ]
    }
   ],
   "source": [
    "#All tweets linearly joined together in a list\n",
    "char_list = \" \".join(list(df[\"message\"]))\n",
    "\n",
    "print(\"Unique space-separated character orderings: {}\".format(len({\n",
    "    word: None for word in char_list.split(\" \")})))\n",
    "print(\"Tweets: {}\".format(df.shape[0]))\n",
    "\n",
    "print(\"Average sentences per tweet: {}\".format(\n",
    "    (char_list.count(\".\") + char_list.count(\"?\") + char_list.count(\"!\")) / float(df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lookup table\n",
    "In order to create a word embedding, the words used in the tweets need to be transformed to IDs. The 2 way mapping from words->IDs and IDs->words is generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"here's some sample text. hopefully this\\nworks? ok - time to give it a shot!!\"\n",
    "def get_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Gets the lookup tables mapping character orderings to their IDs and vice-versa.\n",
    "    :param text: Text to create lookup tables from\n",
    "    :return: A tuple of mapes (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    #If passed in text as big string, words separated by spaces\n",
    "    if type(text) == str:\n",
    "        #text = text.translate(None, punctuation).split()\n",
    "        text = text.split()\n",
    "    #If passed in text as list (same as string representation but separated by indices)\n",
    "    elif type(text) == list:\n",
    "        #Handle later\n",
    "        None\n",
    "        \n",
    "    #Create mappings\n",
    "    words = [k for (k,v) in Counter(text).items()]\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    for i in range(len(words)):\n",
    "        vocab_to_int[words[i]] = i\n",
    "        int_to_vocab[i] = words[i]\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "#get_lookup_tables(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation tokenizing\n",
    "Spaces split the tweets up word by words. However, punctuations make it difficult for neural networks to distinguish between \"dream\" and \"dream!\". The requring tokenization mechanism to map characters to their IDs is performed below.\n",
    "\n",
    "With this mapping mechanism, the dictionary will be used to toeknize the symbols and add a space around the character, making the character it's own word. When punctuations act as their own word, the neural network can more easily incorporate them into it's produced language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': '~NEWLINE~',\n",
       " '!': '~EXCLAMATION~',\n",
       " '\"': '~QUOTATION~',\n",
       " '(': '~LEFTPAREN~',\n",
       " ')': '~RIGHTPAREN~',\n",
       " ',': '~COMMA~',\n",
       " '-': '~HYPHEN~',\n",
       " '.': '~PERIOD~',\n",
       " ';': '~SEMICOLON~',\n",
       " '?': '~QUESTION~',\n",
       " '|': '~PIPE~'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Consider adding possessive/abbreviation for punctuation ... \"'\"\n",
    "rnn_punctuation = [\".\", \",\", \"\\\"\", \";\", \"!\", \"?\", \"(\", \")\", \"-\", \"\\n\", \"|\"]\n",
    "rnn_punctuation_words = list(map(lambda s : \"~\" + s.upper() + \"~\", [\"period\", \"comma\", \"quotation\", \"semicolon\", \"exclamation\",\n",
    "                         \"question\", \"leftparen\", \"rightparen\", \"hyphen\", \"newline\", \"pipe\"]))\n",
    "\n",
    "rnn_punctuation_map = {rnn_punctuation[i]: rnn_punctuation_words[i] for i in range(len(rnn_punctuation))}\n",
    "rnn_punctuation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-004078d1c306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Check tensorflow version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m               \u001b[1;34m\"necessary DLL by downloading cuDNN %s from this URL: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m               \u001b[1;34m\"https://developer.nvidia.com/cudnn\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m               % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "#Check tensorflow version\n",
    "assert LooseVersion(\"1.0\") <= LooseVersion(tf.__version__), \"Please use TensorFlow version 1.0 or newer\"\n",
    "print(\"TensorFlow Version: {}\".format(tf.__version__))\n",
    "\n",
    "#Check for GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn(\"No GPU found. Please use a GPU to train your nerual network.\")\n",
    "else:\n",
    "    print(\"Default GPU device: {}\".format(tf.test.gpu_device_name()))\n",
    "    \n",
    "#Output available devices\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "for d in devices:\n",
    "    print(d.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
