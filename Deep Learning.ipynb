{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "This notebook is responsible for implementing a recurrent neural network using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"localhost\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe-ize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql as pms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great meeting with @Cabinet at the @WhiteHouse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking forward to 3:30 P.M. meeting today at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lowest rated Oscars in HISTORY. Problem is, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>JOBS, JOBS, JOBS! #MAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The U.S. is acting swiftly on Intellectual Pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message\n",
       "0   1  Great meeting with @Cabinet at the @WhiteHouse...\n",
       "1   2  Looking forward to 3:30 P.M. meeting today at ...\n",
       "2   3  Lowest rated Oscars in HISTORY. Problem is, we...\n",
       "3   4                            JOBS, JOBS, JOBS! #MAGA\n",
       "4   5  The U.S. is acting swiftly on Intellectual Pro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    con = pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name)\n",
    "    df = pd.read_sql(\"\"\"SELECT * FROM search_tweets\"\"\", con)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique space-separated character orderings: 12837\n",
      "Tweets: 2889\n",
      "Average sentences per tweet: 2.7608168916580134\n"
     ]
    }
   ],
   "source": [
    "#All tweets linearly joined together in a list\n",
    "char_list = \" \".join(list(df[\"message\"]))\n",
    "\n",
    "print(\"Unique space-separated character orderings: {}\".format(len({\n",
    "    word: None for word in char_list.split(\" \")})))\n",
    "print(\"Tweets: {}\".format(df.shape[0]))\n",
    "\n",
    "print(\"Average sentences per tweet: {}\".format(\n",
    "    (char_list.count(\".\") + char_list.count(\"?\") + char_list.count(\"!\")) / float(df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lookup table\n",
    "In order to create a word embedding, the words used in the tweets need to be transformed to IDs. The 2 way mapping from words->IDs and IDs->words is generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "punc = punctuation.replace(\"#\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"here's some sample text. hopefully this\\nworks? ok - time to give it a shot!!\"\n",
    "def get_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Gets the lookup tables mapping character orderings to their IDs and vice-versa.\n",
    "    :param text: Text to create lookup tables from\n",
    "    :return: A tuple of mapes (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    #If passed in text as big string, words separated by spaces\n",
    "    if type(text) == str:\n",
    "        #text = text.translate(None, punctuation).split()\n",
    "        text = text.split()\n",
    "    #If passed in text as list (same as string representation but separated by indices)\n",
    "    elif type(text) == list:\n",
    "        #Handle later\n",
    "        None\n",
    "        \n",
    "    #Create mappings\n",
    "    words = [k for (k,v) in Counter(text).items()]\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    for i in range(len(words)):\n",
    "        vocab_to_int[words[i]] = i\n",
    "        int_to_vocab[i] = words[i]\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "#get_lookup_tables(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation tokenizing\n",
    "Spaces split the tweets up word by words. However, punctuations make it difficult for neural networks to distinguish between \"dream\" and \"dream!\". The requring tokenization mechanism to map characters to their IDs is performed below.\n",
    "\n",
    "With this mapping mechanism, the dictionary will be used to toeknize the symbols and add a space around the character, making the character it's own word. When punctuations act as their own word, the neural network can more easily incorporate them into it's produced language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': ' ~NEWLINE~',\n",
       " '!': ' ~EXCLAMATION~',\n",
       " '\"': ' ~QUOTATION~',\n",
       " '(': ' ~LEFTPAREN~',\n",
       " ')': ' ~RIGHTPAREN~',\n",
       " ',': ' ~COMMA~',\n",
       " '-': ' ~HYPHEN~',\n",
       " '.': ' ~PERIOD~',\n",
       " ';': ' ~SEMICOLON~',\n",
       " '?': ' ~QUESTION~',\n",
       " '|': ' ~PIPE~'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Consider adding possessive/abbreviation for punctuation ... \"'\"\n",
    "rnn_punctuation = [\".\", \",\", \"\\\"\", \";\", \"!\", \"?\", \"(\", \")\", \"-\", \"\\n\", \"|\"]\n",
    "rnn_punctuation_words = list(map(lambda s : \" ~\" + s.upper() + \"~\", [\"period\", \"comma\", \"quotation\", \"semicolon\", \"exclamation\",\n",
    "                         \"question\", \"leftparen\", \"rightparen\", \"hyphen\", \"newline\", \"pipe\"]))\n",
    "\n",
    "rnn_punctuation_map = {rnn_punctuation[i]: rnn_punctuation_words[i] for i in range(len(rnn_punctuation))}\n",
    "rnn_punctuation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join all text and tokenize the punctuation\n",
    "puncd_text = \" \".join([m for m in df[\"message\"]])\n",
    "filtered_text = \"\"\n",
    "for ch in puncd_text:\n",
    "    filtered_text += ch if ch not in rnn_punctuation_map else rnn_punctuation_map[ch]\n",
    "    \n",
    "#Create the lookup tables\n",
    "vocab_to_int, int_to_vocab = get_lookup_tables(filtered_text)\n",
    "\n",
    "#all_text representation by IDs\n",
    "int_text = [vocab_to_int[vocab] for vocab in filtered_text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN\n",
    "### Checking TensorFlow\n",
    "I'm having difficulty with linking tensorflow-gpu to a CUDA .dll. Because of this I'm just going to run with CPU TensorFlow right now, since the main point of this project is not GPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "/cpu:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: No GPU found. Please use a GPU to train your nerual network.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "#Check tensorflow version\n",
    "assert LooseVersion(\"1.0\") <= LooseVersion(tf.__version__), \"Please use TensorFlow version 1.0 or newer\"\n",
    "print(\"TensorFlow Version: {}\".format(tf.__version__))\n",
    "\n",
    "#Check for GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn(\"No GPU found. Please use a GPU to train your nerual network.\")\n",
    "else:\n",
    "    print(\"Default GPU device: {}\".format(tf.test.gpu_device_name()))\n",
    "    \n",
    "#Output available devices\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "for d in devices:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Create TensorFlow placeholders for the neural network for the input text, targets, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Returns a size 3 tuple of TensorFlow placehodlers for the input text, targets,\n",
    "    and learning rate used by for the RNN.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tf.placeholder(tf.int32, shape=[None, None], name=\"input\"),\n",
    "        tf.placeholder(tf.int32, shape=[None, None], name=\"targets\"),\n",
    "        tf.placeholder(tf.float32, name=\"lr\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell and RNN Size\n",
    "Stack one or more long-short term memory cells using TensorFlow's BasicLSTMCell and MultiRNNCell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Creates an RNN cell and initializes it.\n",
    "    :param batch_size: Size of input batches\n",
    "    :param rnn_size: Size of a MultiRNNCell\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm_size = 256\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm] * rnn_size)\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name=\"initial_state\")\n",
    "    return (cell, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "Apply the word embedding to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Return embedding for input_data.\n",
    "    :param input_data: TensorFlow placeholder for text input\n",
    "    :param vocab_size: Number of words in vocabulary\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the RNN\n",
    "The single RNN cell has already been designed. Now design the recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create an RNN using the RNN cell\n",
    "    :param cell: RNN cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (outputs, final state)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, \"final_state\")\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid the entire Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the NN\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of RNN\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (logits, final_state)\n",
    "    \"\"\"\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embedding)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "Implement a batching function to create batches of input and targets. The batches should be a NumPy array with the shape (number of batches, 2, batch size, sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their IDs\n",
    "    :param batch_size: The size of the batch\n",
    "    :param seq_length: The length of the sequence\n",
    "    :return: Batches as a NumPy array\n",
    "    \"\"\"\n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    #Initialize batches\n",
    "    batches = [[[], []] for i in range(num_batches)]\n",
    "    #Truncate input\n",
    "    int_text = int_text[:batch_size*seq_length*num_batches]\n",
    "    \n",
    "    #Iterate until all elements are used\n",
    "    for i in range(0, len(int_text), seq_length*num_batches):\n",
    "        for j in range(num_batches):\n",
    "            start = i + seq_length * j\n",
    "            end = start + seq_length\n",
    "            curr_in = int_text[start:end]\n",
    "            curr_out = []\n",
    "            for k in range(seq_length):\n",
    "                curr_out.append(int_text[(start + k + 1) % len(int_text)])\n",
    "                batches[j][0].append(curr_in)\n",
    "                batches[j][1].append(curr_out)\n",
    "    return np.array(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 256\n",
    "rnn_size = 1\n",
    "embed_dim = 500\n",
    "seq_length = 15\n",
    "learning_rate = .01\n",
    "show_every_n_batches = 25\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    #Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name=\"probs\")\n",
    "    \n",
    "    #Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]])\n",
    "    )\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    #Gradient clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad,var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/20\t train_loss = 9.167\n",
      "Epoch   0 Batch    1/20\t train_loss = 9.048\n",
      "Epoch   0 Batch    2/20\t train_loss = 8.186\n",
      "Epoch   0 Batch    3/20\t train_loss = 7.467\n",
      "Epoch   0 Batch    4/20\t train_loss = 7.523\n",
      "Epoch   0 Batch    5/20\t train_loss = 7.527\n",
      "Epoch   0 Batch    6/20\t train_loss = 7.412\n",
      "Epoch   0 Batch    7/20\t train_loss = 7.406\n",
      "Epoch   0 Batch    8/20\t train_loss = 7.345\n",
      "Epoch   0 Batch    9/20\t train_loss = 7.260\n",
      "Epoch   0 Batch   10/20\t train_loss = 7.225\n",
      "Epoch   0 Batch   11/20\t train_loss = 6.982\n",
      "Epoch   0 Batch   12/20\t train_loss = 7.047\n",
      "Epoch   0 Batch   13/20\t train_loss = 6.948\n",
      "Epoch   0 Batch   14/20\t train_loss = 6.855\n",
      "Epoch   0 Batch   15/20\t train_loss = 6.836\n",
      "Epoch   0 Batch   16/20\t train_loss = 6.658\n",
      "Epoch   0 Batch   17/20\t train_loss = 6.576\n",
      "Epoch   0 Batch   18/20\t train_loss = 6.665\n",
      "Epoch   0 Batch   19/20\t train_loss = 6.548\n",
      "Epoch   1 Batch    0/20\t train_loss = 6.177\n",
      "Epoch   1 Batch    1/20\t train_loss = 6.142\n",
      "Epoch   1 Batch    2/20\t train_loss = 6.120\n",
      "Epoch   1 Batch    3/20\t train_loss = 6.116\n",
      "Epoch   1 Batch    4/20\t train_loss = 6.112\n",
      "Epoch   1 Batch    5/20\t train_loss = 5.941\n",
      "Epoch   1 Batch    6/20\t train_loss = 5.850\n",
      "Epoch   1 Batch    7/20\t train_loss = 5.950\n",
      "Epoch   1 Batch    8/20\t train_loss = 5.905\n",
      "Epoch   1 Batch    9/20\t train_loss = 5.869\n",
      "Epoch   1 Batch   10/20\t train_loss = 5.848\n",
      "Epoch   1 Batch   11/20\t train_loss = 5.701\n",
      "Epoch   1 Batch   12/20\t train_loss = 5.750\n",
      "Epoch   1 Batch   13/20\t train_loss = 5.714\n",
      "Epoch   1 Batch   14/20\t train_loss = 5.639\n",
      "Epoch   1 Batch   15/20\t train_loss = 5.620\n",
      "Epoch   1 Batch   16/20\t train_loss = 5.489\n",
      "Epoch   1 Batch   17/20\t train_loss = 5.365\n",
      "Epoch   1 Batch   18/20\t train_loss = 5.503\n",
      "Epoch   1 Batch   19/20\t train_loss = 5.424\n",
      "Epoch   2 Batch    0/20\t train_loss = 5.119\n",
      "Epoch   2 Batch    1/20\t train_loss = 5.125\n",
      "Epoch   2 Batch    2/20\t train_loss = 5.144\n",
      "Epoch   2 Batch    3/20\t train_loss = 5.201\n",
      "Epoch   2 Batch    4/20\t train_loss = 5.263\n",
      "Epoch   2 Batch    5/20\t train_loss = 5.132\n",
      "Epoch   2 Batch    6/20\t train_loss = 5.095\n",
      "Epoch   2 Batch    7/20\t train_loss = 5.164\n",
      "Epoch   2 Batch    8/20\t train_loss = 5.089\n",
      "Epoch   2 Batch    9/20\t train_loss = 5.048\n",
      "Epoch   2 Batch   10/20\t train_loss = 5.066\n",
      "Epoch   2 Batch   11/20\t train_loss = 4.952\n",
      "Epoch   2 Batch   12/20\t train_loss = 5.001\n",
      "Epoch   2 Batch   13/20\t train_loss = 4.980\n",
      "Epoch   2 Batch   14/20\t train_loss = 4.901\n",
      "Epoch   2 Batch   15/20\t train_loss = 4.868\n",
      "Epoch   2 Batch   16/20\t train_loss = 4.763\n",
      "Epoch   2 Batch   17/20\t train_loss = 4.644\n",
      "Epoch   2 Batch   18/20\t train_loss = 4.752\n",
      "Epoch   2 Batch   19/20\t train_loss = 4.697\n",
      "Epoch   3 Batch    0/20\t train_loss = 4.432\n",
      "Epoch   3 Batch    1/20\t train_loss = 4.462\n",
      "Epoch   3 Batch    2/20\t train_loss = 4.503\n",
      "Epoch   3 Batch    3/20\t train_loss = 4.561\n",
      "Epoch   3 Batch    4/20\t train_loss = 4.618\n",
      "Epoch   3 Batch    5/20\t train_loss = 4.485\n",
      "Epoch   3 Batch    6/20\t train_loss = 4.466\n",
      "Epoch   3 Batch    7/20\t train_loss = 4.511\n",
      "Epoch   3 Batch    8/20\t train_loss = 4.430\n",
      "Epoch   3 Batch    9/20\t train_loss = 4.396\n",
      "Epoch   3 Batch   10/20\t train_loss = 4.413\n",
      "Epoch   3 Batch   11/20\t train_loss = 4.301\n",
      "Epoch   3 Batch   12/20\t train_loss = 4.347\n",
      "Epoch   3 Batch   13/20\t train_loss = 4.346\n",
      "Epoch   3 Batch   14/20\t train_loss = 4.303\n",
      "Epoch   3 Batch   15/20\t train_loss = 4.250\n",
      "Epoch   3 Batch   16/20\t train_loss = 4.191\n",
      "Epoch   3 Batch   17/20\t train_loss = 4.096\n",
      "Epoch   3 Batch   18/20\t train_loss = 4.182\n",
      "Epoch   3 Batch   19/20\t train_loss = 4.171\n",
      "Epoch   4 Batch    0/20\t train_loss = 3.928\n",
      "Epoch   4 Batch    1/20\t train_loss = 3.935\n",
      "Epoch   4 Batch    2/20\t train_loss = 3.951\n",
      "Epoch   4 Batch    3/20\t train_loss = 3.993\n",
      "Epoch   4 Batch    4/20\t train_loss = 4.051\n",
      "Epoch   4 Batch    5/20\t train_loss = 3.941\n",
      "Epoch   4 Batch    6/20\t train_loss = 3.914\n",
      "Epoch   4 Batch    7/20\t train_loss = 3.938\n",
      "Epoch   4 Batch    8/20\t train_loss = 3.884\n",
      "Epoch   4 Batch    9/20\t train_loss = 3.823\n",
      "Epoch   4 Batch   10/20\t train_loss = 3.850\n",
      "Epoch   4 Batch   11/20\t train_loss = 3.792\n",
      "Epoch   4 Batch   12/20\t train_loss = 3.810\n",
      "Epoch   4 Batch   13/20\t train_loss = 3.817\n",
      "Epoch   4 Batch   14/20\t train_loss = 3.793\n",
      "Epoch   4 Batch   15/20\t train_loss = 3.726\n",
      "Epoch   4 Batch   16/20\t train_loss = 3.710\n",
      "Epoch   4 Batch   17/20\t train_loss = 3.614\n",
      "Epoch   4 Batch   18/20\t train_loss = 3.659\n",
      "Epoch   4 Batch   19/20\t train_loss = 3.679\n",
      "Epoch   5 Batch    0/20\t train_loss = 3.473\n",
      "Epoch   5 Batch    1/20\t train_loss = 3.476\n",
      "Epoch   5 Batch    2/20\t train_loss = 3.522\n",
      "Epoch   5 Batch    3/20\t train_loss = 3.530\n",
      "Epoch   5 Batch    4/20\t train_loss = 3.556\n",
      "Epoch   5 Batch    5/20\t train_loss = 3.505\n",
      "Epoch   5 Batch    6/20\t train_loss = 3.498\n",
      "Epoch   5 Batch    7/20\t train_loss = 3.513\n",
      "Epoch   5 Batch    8/20\t train_loss = 3.451\n",
      "Epoch   5 Batch    9/20\t train_loss = 3.378\n",
      "Epoch   5 Batch   10/20\t train_loss = 3.386\n",
      "Epoch   5 Batch   11/20\t train_loss = 3.347\n",
      "Epoch   5 Batch   12/20\t train_loss = 3.326\n",
      "Epoch   5 Batch   13/20\t train_loss = 3.331\n",
      "Epoch   5 Batch   14/20\t train_loss = 3.329\n",
      "Epoch   5 Batch   15/20\t train_loss = 3.266\n",
      "Epoch   5 Batch   16/20\t train_loss = 3.290\n",
      "Epoch   5 Batch   17/20\t train_loss = 3.205\n",
      "Epoch   5 Batch   18/20\t train_loss = 3.225\n",
      "Epoch   5 Batch   19/20\t train_loss = 3.261\n",
      "Epoch   6 Batch    0/20\t train_loss = 3.080\n",
      "Epoch   6 Batch    1/20\t train_loss = 3.078\n",
      "Epoch   6 Batch    2/20\t train_loss = 3.133\n",
      "Epoch   6 Batch    3/20\t train_loss = 3.122\n",
      "Epoch   6 Batch    4/20\t train_loss = 3.136\n",
      "Epoch   6 Batch    5/20\t train_loss = 3.115\n",
      "Epoch   6 Batch    6/20\t train_loss = 3.137\n",
      "Epoch   6 Batch    7/20\t train_loss = 3.139\n",
      "Epoch   6 Batch    8/20\t train_loss = 3.082\n",
      "Epoch   6 Batch    9/20\t train_loss = 3.008\n",
      "Epoch   6 Batch   10/20\t train_loss = 3.005\n",
      "Epoch   6 Batch   11/20\t train_loss = 2.988\n",
      "Epoch   6 Batch   12/20\t train_loss = 2.957\n",
      "Epoch   6 Batch   13/20\t train_loss = 2.948\n",
      "Epoch   6 Batch   14/20\t train_loss = 2.934\n",
      "Epoch   6 Batch   15/20\t train_loss = 2.882\n",
      "Epoch   6 Batch   16/20\t train_loss = 2.933\n",
      "Epoch   6 Batch   17/20\t train_loss = 2.867\n",
      "Epoch   6 Batch   18/20\t train_loss = 2.869\n",
      "Epoch   6 Batch   19/20\t train_loss = 2.945\n",
      "Epoch   7 Batch    0/20\t train_loss = 2.794\n",
      "Epoch   7 Batch    1/20\t train_loss = 2.794\n",
      "Epoch   7 Batch    2/20\t train_loss = 2.864\n",
      "Epoch   7 Batch    3/20\t train_loss = 2.838\n",
      "Epoch   7 Batch    4/20\t train_loss = 2.837\n",
      "Epoch   7 Batch    5/20\t train_loss = 2.829\n",
      "Epoch   7 Batch    6/20\t train_loss = 2.828\n",
      "Epoch   7 Batch    7/20\t train_loss = 2.823\n",
      "Epoch   7 Batch    8/20\t train_loss = 2.776\n",
      "Epoch   7 Batch    9/20\t train_loss = 2.716\n",
      "Epoch   7 Batch   10/20\t train_loss = 2.704\n",
      "Epoch   7 Batch   11/20\t train_loss = 2.714\n",
      "Epoch   7 Batch   12/20\t train_loss = 2.679\n",
      "Epoch   7 Batch   13/20\t train_loss = 2.678\n",
      "Epoch   7 Batch   14/20\t train_loss = 2.653\n",
      "Epoch   7 Batch   15/20\t train_loss = 2.596\n",
      "Epoch   7 Batch   16/20\t train_loss = 2.647\n",
      "Epoch   7 Batch   17/20\t train_loss = 2.587\n",
      "Epoch   7 Batch   18/20\t train_loss = 2.577\n",
      "Epoch   7 Batch   19/20\t train_loss = 2.657\n",
      "Epoch   8 Batch    0/20\t train_loss = 2.533\n",
      "Epoch   8 Batch    1/20\t train_loss = 2.546\n",
      "Epoch   8 Batch    2/20\t train_loss = 2.580\n",
      "Epoch   8 Batch    3/20\t train_loss = 2.578\n",
      "Epoch   8 Batch    4/20\t train_loss = 2.555\n",
      "Epoch   8 Batch    5/20\t train_loss = 2.569\n",
      "Epoch   8 Batch    6/20\t train_loss = 2.578\n",
      "Epoch   8 Batch    7/20\t train_loss = 2.566\n",
      "Epoch   8 Batch    8/20\t train_loss = 2.543\n",
      "Epoch   8 Batch    9/20\t train_loss = 2.471\n",
      "Epoch   8 Batch   10/20\t train_loss = 2.438\n",
      "Epoch   8 Batch   11/20\t train_loss = 2.467\n",
      "Epoch   8 Batch   12/20\t train_loss = 2.433\n",
      "Epoch   8 Batch   13/20\t train_loss = 2.425\n",
      "Epoch   8 Batch   14/20\t train_loss = 2.396\n",
      "Epoch   8 Batch   15/20\t train_loss = 2.362\n",
      "Epoch   8 Batch   16/20\t train_loss = 2.402\n",
      "Epoch   8 Batch   17/20\t train_loss = 2.355\n",
      "Epoch   8 Batch   18/20\t train_loss = 2.322\n",
      "Epoch   8 Batch   19/20\t train_loss = 2.399\n",
      "Epoch   9 Batch    0/20\t train_loss = 2.284\n",
      "Epoch   9 Batch    1/20\t train_loss = 2.299\n",
      "Epoch   9 Batch    2/20\t train_loss = 2.353\n",
      "Epoch   9 Batch    3/20\t train_loss = 2.350\n",
      "Epoch   9 Batch    4/20\t train_loss = 2.329\n",
      "Epoch   9 Batch    5/20\t train_loss = 2.341\n",
      "Epoch   9 Batch    6/20\t train_loss = 2.361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch    7/20\t train_loss = 2.334\n",
      "Epoch   9 Batch    8/20\t train_loss = 2.325\n",
      "Epoch   9 Batch    9/20\t train_loss = 2.260\n",
      "Epoch   9 Batch   10/20\t train_loss = 2.228\n",
      "Epoch   9 Batch   11/20\t train_loss = 2.248\n",
      "Epoch   9 Batch   12/20\t train_loss = 2.213\n",
      "Epoch   9 Batch   13/20\t train_loss = 2.223\n",
      "Epoch   9 Batch   14/20\t train_loss = 2.174\n",
      "Epoch   9 Batch   15/20\t train_loss = 2.140\n",
      "Epoch   9 Batch   16/20\t train_loss = 2.183\n",
      "Epoch   9 Batch   17/20\t train_loss = 2.152\n",
      "Epoch   9 Batch   18/20\t train_loss = 2.102\n",
      "Epoch   9 Batch   19/20\t train_loss = 2.184\n",
      "Epoch  10 Batch    0/20\t train_loss = 2.094\n",
      "Epoch  10 Batch    1/20\t train_loss = 2.097\n",
      "Epoch  10 Batch    2/20\t train_loss = 2.140\n",
      "Epoch  10 Batch    3/20\t train_loss = 2.137\n",
      "Epoch  10 Batch    4/20\t train_loss = 2.096\n",
      "Epoch  10 Batch    5/20\t train_loss = 2.124\n",
      "Epoch  10 Batch    6/20\t train_loss = 2.136\n",
      "Epoch  10 Batch    7/20\t train_loss = 2.119\n",
      "Epoch  10 Batch    8/20\t train_loss = 2.103\n",
      "Epoch  10 Batch    9/20\t train_loss = 2.050\n",
      "Epoch  10 Batch   10/20\t train_loss = 2.007\n",
      "Epoch  10 Batch   11/20\t train_loss = 2.051\n",
      "Epoch  10 Batch   12/20\t train_loss = 2.041\n",
      "Epoch  10 Batch   13/20\t train_loss = 2.035\n",
      "Epoch  10 Batch   14/20\t train_loss = 1.978\n",
      "Epoch  10 Batch   15/20\t train_loss = 1.932\n",
      "Epoch  10 Batch   16/20\t train_loss = 1.993\n",
      "Epoch  10 Batch   17/20\t train_loss = 1.970\n",
      "Epoch  10 Batch   18/20\t train_loss = 1.906\n",
      "Epoch  10 Batch   19/20\t train_loss = 1.984\n",
      "Epoch  11 Batch    0/20\t train_loss = 1.893\n",
      "Epoch  11 Batch    1/20\t train_loss = 1.903\n",
      "Epoch  11 Batch    2/20\t train_loss = 1.960\n",
      "Epoch  11 Batch    3/20\t train_loss = 1.933\n",
      "Epoch  11 Batch    4/20\t train_loss = 1.906\n",
      "Epoch  11 Batch    5/20\t train_loss = 1.921\n",
      "Epoch  11 Batch    6/20\t train_loss = 1.949\n",
      "Epoch  11 Batch    7/20\t train_loss = 1.921\n",
      "Epoch  11 Batch    8/20\t train_loss = 1.897\n",
      "Epoch  11 Batch    9/20\t train_loss = 1.855\n",
      "Epoch  11 Batch   10/20\t train_loss = 1.802\n",
      "Epoch  11 Batch   11/20\t train_loss = 1.844\n",
      "Epoch  11 Batch   12/20\t train_loss = 1.835\n",
      "Epoch  11 Batch   13/20\t train_loss = 1.843\n",
      "Epoch  11 Batch   14/20\t train_loss = 1.797\n",
      "Epoch  11 Batch   15/20\t train_loss = 1.737\n",
      "Epoch  11 Batch   16/20\t train_loss = 1.806\n",
      "Epoch  11 Batch   17/20\t train_loss = 1.801\n",
      "Epoch  11 Batch   18/20\t train_loss = 1.740\n",
      "Epoch  11 Batch   19/20\t train_loss = 1.807\n",
      "Epoch  12 Batch    0/20\t train_loss = 1.722\n",
      "Epoch  12 Batch    1/20\t train_loss = 1.739\n",
      "Epoch  12 Batch    2/20\t train_loss = 1.797\n",
      "Epoch  12 Batch    3/20\t train_loss = 1.759\n",
      "Epoch  12 Batch    4/20\t train_loss = 1.723\n",
      "Epoch  12 Batch    5/20\t train_loss = 1.752\n",
      "Epoch  12 Batch    6/20\t train_loss = 1.756\n",
      "Epoch  12 Batch    7/20\t train_loss = 1.754\n",
      "Epoch  12 Batch    8/20\t train_loss = 1.727\n",
      "Epoch  12 Batch    9/20\t train_loss = 1.675\n",
      "Epoch  12 Batch   10/20\t train_loss = 1.622\n",
      "Epoch  12 Batch   11/20\t train_loss = 1.655\n",
      "Epoch  12 Batch   12/20\t train_loss = 1.666\n",
      "Epoch  12 Batch   13/20\t train_loss = 1.655\n",
      "Epoch  12 Batch   14/20\t train_loss = 1.623\n",
      "Epoch  12 Batch   15/20\t train_loss = 1.584\n",
      "Epoch  12 Batch   16/20\t train_loss = 1.632\n",
      "Epoch  12 Batch   17/20\t train_loss = 1.637\n",
      "Epoch  12 Batch   18/20\t train_loss = 1.568\n",
      "Epoch  12 Batch   19/20\t train_loss = 1.645\n",
      "Epoch  13 Batch    0/20\t train_loss = 1.581\n",
      "Epoch  13 Batch    1/20\t train_loss = 1.589\n",
      "Epoch  13 Batch    2/20\t train_loss = 1.625\n",
      "Epoch  13 Batch    3/20\t train_loss = 1.601\n",
      "Epoch  13 Batch    4/20\t train_loss = 1.567\n",
      "Epoch  13 Batch    5/20\t train_loss = 1.592\n",
      "Epoch  13 Batch    6/20\t train_loss = 1.602\n",
      "Epoch  13 Batch    7/20\t train_loss = 1.589\n",
      "Epoch  13 Batch    8/20\t train_loss = 1.560\n",
      "Epoch  13 Batch    9/20\t train_loss = 1.522\n",
      "Epoch  13 Batch   10/20\t train_loss = 1.469\n",
      "Epoch  13 Batch   11/20\t train_loss = 1.533\n",
      "Epoch  13 Batch   12/20\t train_loss = 1.533\n",
      "Epoch  13 Batch   13/20\t train_loss = 1.533\n",
      "Epoch  13 Batch   14/20\t train_loss = 1.485\n",
      "Epoch  13 Batch   15/20\t train_loss = 1.429\n",
      "Epoch  13 Batch   16/20\t train_loss = 1.473\n",
      "Epoch  13 Batch   17/20\t train_loss = 1.478\n",
      "Epoch  13 Batch   18/20\t train_loss = 1.409\n",
      "Epoch  13 Batch   19/20\t train_loss = 1.496\n",
      "Epoch  14 Batch    0/20\t train_loss = 1.426\n",
      "Epoch  14 Batch    1/20\t train_loss = 1.461\n",
      "Epoch  14 Batch    2/20\t train_loss = 1.479\n",
      "Epoch  14 Batch    3/20\t train_loss = 1.463\n",
      "Epoch  14 Batch    4/20\t train_loss = 1.433\n",
      "Epoch  14 Batch    5/20\t train_loss = 1.453\n",
      "Epoch  14 Batch    6/20\t train_loss = 1.476\n",
      "Epoch  14 Batch    7/20\t train_loss = 1.445\n",
      "Epoch  14 Batch    8/20\t train_loss = 1.412\n",
      "Epoch  14 Batch    9/20\t train_loss = 1.376\n",
      "Epoch  14 Batch   10/20\t train_loss = 1.327\n",
      "Epoch  14 Batch   11/20\t train_loss = 1.386\n",
      "Epoch  14 Batch   12/20\t train_loss = 1.386\n",
      "Epoch  14 Batch   13/20\t train_loss = 1.374\n",
      "Epoch  14 Batch   14/20\t train_loss = 1.336\n",
      "Epoch  14 Batch   15/20\t train_loss = 1.299\n",
      "Epoch  14 Batch   16/20\t train_loss = 1.357\n",
      "Epoch  14 Batch   17/20\t train_loss = 1.358\n",
      "Epoch  14 Batch   18/20\t train_loss = 1.277\n",
      "Epoch  14 Batch   19/20\t train_loss = 1.352\n",
      "Epoch  15 Batch    0/20\t train_loss = 1.299\n",
      "Epoch  15 Batch    1/20\t train_loss = 1.319\n",
      "Epoch  15 Batch    2/20\t train_loss = 1.343\n",
      "Epoch  15 Batch    3/20\t train_loss = 1.308\n",
      "Epoch  15 Batch    4/20\t train_loss = 1.293\n",
      "Epoch  15 Batch    5/20\t train_loss = 1.306\n",
      "Epoch  15 Batch    6/20\t train_loss = 1.341\n",
      "Epoch  15 Batch    7/20\t train_loss = 1.317\n",
      "Epoch  15 Batch    8/20\t train_loss = 1.298\n",
      "Epoch  15 Batch    9/20\t train_loss = 1.268\n",
      "Epoch  15 Batch   10/20\t train_loss = 1.216\n",
      "Epoch  15 Batch   11/20\t train_loss = 1.269\n",
      "Epoch  15 Batch   12/20\t train_loss = 1.258\n",
      "Epoch  15 Batch   13/20\t train_loss = 1.255\n",
      "Epoch  15 Batch   14/20\t train_loss = 1.224\n",
      "Epoch  15 Batch   15/20\t train_loss = 1.183\n",
      "Epoch  15 Batch   16/20\t train_loss = 1.226\n",
      "Epoch  15 Batch   17/20\t train_loss = 1.245\n",
      "Epoch  15 Batch   18/20\t train_loss = 1.165\n",
      "Epoch  15 Batch   19/20\t train_loss = 1.233\n",
      "Epoch  16 Batch    0/20\t train_loss = 1.166\n",
      "Epoch  16 Batch    1/20\t train_loss = 1.207\n",
      "Epoch  16 Batch    2/20\t train_loss = 1.221\n",
      "Epoch  16 Batch    3/20\t train_loss = 1.198\n",
      "Epoch  16 Batch    4/20\t train_loss = 1.191\n",
      "Epoch  16 Batch    5/20\t train_loss = 1.205\n",
      "Epoch  16 Batch    6/20\t train_loss = 1.238\n",
      "Epoch  16 Batch    7/20\t train_loss = 1.205\n",
      "Epoch  16 Batch    8/20\t train_loss = 1.178\n",
      "Epoch  16 Batch    9/20\t train_loss = 1.161\n",
      "Epoch  16 Batch   10/20\t train_loss = 1.112\n",
      "Epoch  16 Batch   11/20\t train_loss = 1.166\n",
      "Epoch  16 Batch   12/20\t train_loss = 1.154\n",
      "Epoch  16 Batch   13/20\t train_loss = 1.140\n",
      "Epoch  16 Batch   14/20\t train_loss = 1.116\n",
      "Epoch  16 Batch   15/20\t train_loss = 1.081\n",
      "Epoch  16 Batch   16/20\t train_loss = 1.118\n",
      "Epoch  16 Batch   17/20\t train_loss = 1.124\n",
      "Epoch  16 Batch   18/20\t train_loss = 1.058\n",
      "Epoch  16 Batch   19/20\t train_loss = 1.133\n",
      "Epoch  17 Batch    0/20\t train_loss = 1.071\n",
      "Epoch  17 Batch    1/20\t train_loss = 1.097\n",
      "Epoch  17 Batch    2/20\t train_loss = 1.111\n",
      "Epoch  17 Batch    3/20\t train_loss = 1.082\n",
      "Epoch  17 Batch    4/20\t train_loss = 1.075\n",
      "Epoch  17 Batch    5/20\t train_loss = 1.101\n",
      "Epoch  17 Batch    6/20\t train_loss = 1.128\n",
      "Epoch  17 Batch    7/20\t train_loss = 1.111\n",
      "Epoch  17 Batch    8/20\t train_loss = 1.092\n",
      "Epoch  17 Batch    9/20\t train_loss = 1.061\n",
      "Epoch  17 Batch   10/20\t train_loss = 1.025\n",
      "Epoch  17 Batch   11/20\t train_loss = 1.075\n",
      "Epoch  17 Batch   12/20\t train_loss = 1.051\n",
      "Epoch  17 Batch   13/20\t train_loss = 1.042\n",
      "Epoch  17 Batch   14/20\t train_loss = 1.020\n",
      "Epoch  17 Batch   15/20\t train_loss = 0.976\n",
      "Epoch  17 Batch   16/20\t train_loss = 1.030\n",
      "Epoch  17 Batch   17/20\t train_loss = 1.026\n",
      "Epoch  17 Batch   18/20\t train_loss = 0.958\n",
      "Epoch  17 Batch   19/20\t train_loss = 1.026\n",
      "Epoch  18 Batch    0/20\t train_loss = 0.982\n",
      "Epoch  18 Batch    1/20\t train_loss = 1.013\n",
      "Epoch  18 Batch    2/20\t train_loss = 1.038\n",
      "Epoch  18 Batch    3/20\t train_loss = 1.012\n",
      "Epoch  18 Batch    4/20\t train_loss = 0.969\n",
      "Epoch  18 Batch    5/20\t train_loss = 0.994\n",
      "Epoch  18 Batch    6/20\t train_loss = 1.042\n",
      "Epoch  18 Batch    7/20\t train_loss = 1.012\n",
      "Epoch  18 Batch    8/20\t train_loss = 0.991\n",
      "Epoch  18 Batch    9/20\t train_loss = 0.977\n",
      "Epoch  18 Batch   10/20\t train_loss = 0.937\n",
      "Epoch  18 Batch   11/20\t train_loss = 0.991\n",
      "Epoch  18 Batch   12/20\t train_loss = 0.966\n",
      "Epoch  18 Batch   13/20\t train_loss = 0.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18 Batch   14/20\t train_loss = 0.943\n",
      "Epoch  18 Batch   15/20\t train_loss = 0.909\n",
      "Epoch  18 Batch   16/20\t train_loss = 0.938\n",
      "Epoch  18 Batch   17/20\t train_loss = 0.953\n",
      "Epoch  18 Batch   18/20\t train_loss = 0.863\n",
      "Epoch  18 Batch   19/20\t train_loss = 0.924\n",
      "Epoch  19 Batch    0/20\t train_loss = 0.878\n",
      "Epoch  19 Batch    1/20\t train_loss = 0.921\n",
      "Epoch  19 Batch    2/20\t train_loss = 0.947\n",
      "Epoch  19 Batch    3/20\t train_loss = 0.927\n",
      "Epoch  19 Batch    4/20\t train_loss = 0.896\n",
      "Epoch  19 Batch    5/20\t train_loss = 0.922\n",
      "Epoch  19 Batch    6/20\t train_loss = 0.966\n",
      "Epoch  19 Batch    7/20\t train_loss = 0.925\n",
      "Epoch  19 Batch    8/20\t train_loss = 0.887\n",
      "Epoch  19 Batch    9/20\t train_loss = 0.876\n",
      "Epoch  19 Batch   10/20\t train_loss = 0.857\n",
      "Epoch  19 Batch   11/20\t train_loss = 0.896\n",
      "Epoch  19 Batch   12/20\t train_loss = 0.896\n",
      "Epoch  19 Batch   13/20\t train_loss = 0.899\n",
      "Epoch  19 Batch   14/20\t train_loss = 0.868\n",
      "Epoch  19 Batch   15/20\t train_loss = 0.828\n",
      "Epoch  19 Batch   16/20\t train_loss = 0.862\n",
      "Epoch  19 Batch   17/20\t train_loss = 0.873\n",
      "Epoch  19 Batch   18/20\t train_loss = 0.794\n",
      "Epoch  19 Batch   19/20\t train_loss = 0.834\n",
      "Epoch  20 Batch    0/20\t train_loss = 0.794\n",
      "Epoch  20 Batch    1/20\t train_loss = 0.850\n",
      "Epoch  20 Batch    2/20\t train_loss = 0.845\n",
      "Epoch  20 Batch    3/20\t train_loss = 0.834\n",
      "Epoch  20 Batch    4/20\t train_loss = 0.791\n",
      "Epoch  20 Batch    5/20\t train_loss = 0.824\n",
      "Epoch  20 Batch    6/20\t train_loss = 0.864\n",
      "Epoch  20 Batch    7/20\t train_loss = 0.849\n",
      "Epoch  20 Batch    8/20\t train_loss = 0.812\n",
      "Epoch  20 Batch    9/20\t train_loss = 0.805\n",
      "Epoch  20 Batch   10/20\t train_loss = 0.773\n",
      "Epoch  20 Batch   11/20\t train_loss = 0.820\n",
      "Epoch  20 Batch   12/20\t train_loss = 0.807\n",
      "Epoch  20 Batch   13/20\t train_loss = 0.808\n",
      "Epoch  20 Batch   14/20\t train_loss = 0.783\n",
      "Epoch  20 Batch   15/20\t train_loss = 0.747\n",
      "Epoch  20 Batch   16/20\t train_loss = 0.786\n",
      "Epoch  20 Batch   17/20\t train_loss = 0.794\n",
      "Epoch  20 Batch   18/20\t train_loss = 0.730\n",
      "Epoch  20 Batch   19/20\t train_loss = 0.763\n",
      "Epoch  21 Batch    0/20\t train_loss = 0.737\n",
      "Epoch  21 Batch    1/20\t train_loss = 0.768\n",
      "Epoch  21 Batch    2/20\t train_loss = 0.785\n",
      "Epoch  21 Batch    3/20\t train_loss = 0.753\n",
      "Epoch  21 Batch    4/20\t train_loss = 0.705\n",
      "Epoch  21 Batch    5/20\t train_loss = 0.742\n",
      "Epoch  21 Batch    6/20\t train_loss = 0.784\n",
      "Epoch  21 Batch    7/20\t train_loss = 0.747\n",
      "Epoch  21 Batch    8/20\t train_loss = 0.724\n",
      "Epoch  21 Batch    9/20\t train_loss = 0.721\n",
      "Epoch  21 Batch   10/20\t train_loss = 0.699\n",
      "Epoch  21 Batch   11/20\t train_loss = 0.749\n",
      "Epoch  21 Batch   12/20\t train_loss = 0.723\n",
      "Epoch  21 Batch   13/20\t train_loss = 0.733\n",
      "Epoch  21 Batch   14/20\t train_loss = 0.720\n",
      "Epoch  21 Batch   15/20\t train_loss = 0.683\n",
      "Epoch  21 Batch   16/20\t train_loss = 0.706\n",
      "Epoch  21 Batch   17/20\t train_loss = 0.725\n",
      "Epoch  21 Batch   18/20\t train_loss = 0.653\n",
      "Epoch  21 Batch   19/20\t train_loss = 0.683\n",
      "Epoch  22 Batch    0/20\t train_loss = 0.665\n",
      "Epoch  22 Batch    1/20\t train_loss = 0.704\n",
      "Epoch  22 Batch    2/20\t train_loss = 0.696\n",
      "Epoch  22 Batch    3/20\t train_loss = 0.689\n",
      "Epoch  22 Batch    4/20\t train_loss = 0.646\n",
      "Epoch  22 Batch    5/20\t train_loss = 0.680\n",
      "Epoch  22 Batch    6/20\t train_loss = 0.701\n",
      "Epoch  22 Batch    7/20\t train_loss = 0.678\n",
      "Epoch  22 Batch    8/20\t train_loss = 0.648\n",
      "Epoch  22 Batch    9/20\t train_loss = 0.649\n",
      "Epoch  22 Batch   10/20\t train_loss = 0.624\n",
      "Epoch  22 Batch   11/20\t train_loss = 0.676\n",
      "Epoch  22 Batch   12/20\t train_loss = 0.649\n",
      "Epoch  22 Batch   13/20\t train_loss = 0.656\n",
      "Epoch  22 Batch   14/20\t train_loss = 0.646\n",
      "Epoch  22 Batch   15/20\t train_loss = 0.601\n",
      "Epoch  22 Batch   16/20\t train_loss = 0.644\n",
      "Epoch  22 Batch   17/20\t train_loss = 0.651\n",
      "Epoch  22 Batch   18/20\t train_loss = 0.580\n",
      "Epoch  22 Batch   19/20\t train_loss = 0.607\n",
      "Epoch  23 Batch    0/20\t train_loss = 0.583\n",
      "Epoch  23 Batch    1/20\t train_loss = 0.638\n",
      "Epoch  23 Batch    2/20\t train_loss = 0.628\n",
      "Epoch  23 Batch    3/20\t train_loss = 0.609\n",
      "Epoch  23 Batch    4/20\t train_loss = 0.592\n",
      "Epoch  23 Batch    5/20\t train_loss = 0.598\n",
      "Epoch  23 Batch    6/20\t train_loss = 0.627\n",
      "Epoch  23 Batch    7/20\t train_loss = 0.602\n",
      "Epoch  23 Batch    8/20\t train_loss = 0.572\n",
      "Epoch  23 Batch    9/20\t train_loss = 0.582\n",
      "Epoch  23 Batch   10/20\t train_loss = 0.561\n",
      "Epoch  23 Batch   11/20\t train_loss = 0.604\n",
      "Epoch  23 Batch   12/20\t train_loss = 0.588\n",
      "Epoch  23 Batch   13/20\t train_loss = 0.584\n",
      "Epoch  23 Batch   14/20\t train_loss = 0.572\n",
      "Epoch  23 Batch   15/20\t train_loss = 0.535\n",
      "Epoch  23 Batch   16/20\t train_loss = 0.565\n",
      "Epoch  23 Batch   17/20\t train_loss = 0.581\n",
      "Epoch  23 Batch   18/20\t train_loss = 0.515\n",
      "Epoch  23 Batch   19/20\t train_loss = 0.545\n",
      "Epoch  24 Batch    0/20\t train_loss = 0.530\n",
      "Epoch  24 Batch    1/20\t train_loss = 0.567\n",
      "Epoch  24 Batch    2/20\t train_loss = 0.561\n",
      "Epoch  24 Batch    3/20\t train_loss = 0.554\n",
      "Epoch  24 Batch    4/20\t train_loss = 0.531\n",
      "Epoch  24 Batch    5/20\t train_loss = 0.543\n",
      "Epoch  24 Batch    6/20\t train_loss = 0.574\n",
      "Epoch  24 Batch    7/20\t train_loss = 0.542\n",
      "Epoch  24 Batch    8/20\t train_loss = 0.508\n",
      "Epoch  24 Batch    9/20\t train_loss = 0.520\n",
      "Epoch  24 Batch   10/20\t train_loss = 0.496\n",
      "Epoch  24 Batch   11/20\t train_loss = 0.533\n",
      "Epoch  24 Batch   12/20\t train_loss = 0.519\n",
      "Epoch  24 Batch   13/20\t train_loss = 0.519\n",
      "Epoch  24 Batch   14/20\t train_loss = 0.513\n",
      "Epoch  24 Batch   15/20\t train_loss = 0.492\n",
      "Epoch  24 Batch   16/20\t train_loss = 0.515\n",
      "Epoch  24 Batch   17/20\t train_loss = 0.528\n",
      "Epoch  24 Batch   18/20\t train_loss = 0.468\n",
      "Epoch  24 Batch   19/20\t train_loss = 0.484\n",
      "Model trained and saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_i, (x,y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text:x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            \n",
    "            #Show every N batches\n",
    "            print(\"Epoch {:>3} Batch {:>4}/{}\\t train_loss = {:.3f}\".format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss\n",
    "                ))\n",
    "            \"\"\"if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print(\"Epoch {:>3} Batch {:>4}/{}\\t train_loss = {:.3f}\".format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss\n",
    "                ))\n",
    "                \"\"\"\n",
    "    #Save\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print(\"Model trained and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
