{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "This notebook is responsible for implementing a recurrent neural network using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_user = \"\"\n",
    "db_pass = \"\"\n",
    "db_name = \"\"\n",
    "db_host = \"localhost\"\n",
    "with open(\"database_credentials.txt\") as f:\n",
    "    db_user = f.readline().strip()\n",
    "    db_pass = f.readline().strip()\n",
    "    db_name = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe-ize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql as pms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great meeting with @Cabinet at the @WhiteHouse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking forward to 3:30 P.M. meeting today at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lowest rated Oscars in HISTORY. Problem is, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>JOBS, JOBS, JOBS! #MAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The U.S. is acting swiftly on Intellectual Pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message\n",
       "0   1  Great meeting with @Cabinet at the @WhiteHouse...\n",
       "1   2  Looking forward to 3:30 P.M. meeting today at ...\n",
       "2   3  Lowest rated Oscars in HISTORY. Problem is, we...\n",
       "3   4                            JOBS, JOBS, JOBS! #MAGA\n",
       "4   5  The U.S. is acting swiftly on Intellectual Pro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    con = pms.connect(host=db_host, user=db_user, passwd=db_pass, db=db_name)\n",
    "    df = pd.read_sql(\"\"\"SELECT * FROM search_tweets\"\"\", con)\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique space-separated character orderings: 12837\n",
      "Tweets: 2889\n",
      "Average sentences per tweet: 2.7608168916580134\n"
     ]
    }
   ],
   "source": [
    "#All tweets linearly joined together in a list\n",
    "char_list = \" \".join(list(df[\"message\"]))\n",
    "\n",
    "print(\"Unique space-separated character orderings: {}\".format(len({\n",
    "    word: None for word in char_list.split(\" \")})))\n",
    "print(\"Tweets: {}\".format(df.shape[0]))\n",
    "\n",
    "print(\"Average sentences per tweet: {}\".format(\n",
    "    (char_list.count(\".\") + char_list.count(\"?\") + char_list.count(\"!\")) / float(df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Lookup table\n",
    "In order to create a word embedding, the words used in the tweets need to be transformed to IDs. The 2 way mapping from words->IDs and IDs->words is generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = \"here's some sample text. hopefully this\\nworks? ok - time to give it a shot!!\"\n",
    "def get_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Gets the lookup tables mapping character orderings to their IDs and vice-versa.\n",
    "    :param text: Text to create lookup tables from\n",
    "    :return: A tuple of mapes (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    #If passed in text as big string, words separated by spaces\n",
    "    if type(text) == str:\n",
    "        #text = text.translate(None, punctuation).split()\n",
    "        text = text.split()\n",
    "    #If passed in text as list (same as string representation but separated by indices)\n",
    "    elif type(text) == list:\n",
    "        #Handle later\n",
    "        None\n",
    "        \n",
    "    #Create mappings\n",
    "    words = [k for (k,v) in Counter(text).items()]\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    for i in range(len(words)):\n",
    "        vocab_to_int[words[i]] = i\n",
    "        int_to_vocab[i] = words[i]\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "#get_lookup_tables(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation tokenizing\n",
    "Spaces split the tweets up word by words. However, punctuations make it difficult for neural networks to distinguish between \"dream\" and \"dream!\". The requring tokenization mechanism to map characters to their IDs is performed below.\n",
    "\n",
    "With this mapping mechanism, the dictionary will be used to toeknize the symbols and add a space around the character, making the character it's own word. When punctuations act as their own word, the neural network can more easily incorporate them into it's produced language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': '~NEWLINE~',\n",
       " '!': '~EXCLAMATION~',\n",
       " '\"': '~QUOTATION~',\n",
       " '(': '~LEFTPAREN~',\n",
       " ')': '~RIGHTPAREN~',\n",
       " ',': '~COMMA~',\n",
       " '-': '~HYPHEN~',\n",
       " '.': '~PERIOD~',\n",
       " ';': '~SEMICOLON~',\n",
       " '?': '~QUESTION~',\n",
       " '|': '~PIPE~'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Consider adding possessive/abbreviation for punctuation ... \"'\"\n",
    "rnn_punctuation = [\".\", \",\", \"\\\"\", \";\", \"!\", \"?\", \"(\", \")\", \"-\", \"\\n\", \"|\"]\n",
    "rnn_punctuation_words = list(map(lambda s : \"~\" + s.upper() + \"~\", [\"period\", \"comma\", \"quotation\", \"semicolon\", \"exclamation\",\n",
    "                         \"question\", \"leftparen\", \"rightparen\", \"hyphen\", \"newline\", \"pipe\"]))\n",
    "\n",
    "rnn_punctuation_map = {rnn_punctuation[i]: rnn_punctuation_words[i] for i in range(len(rnn_punctuation))}\n",
    "rnn_punctuation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN\n",
    "### Checking TensorFlow\n",
    "I'm having difficulty with linking tensorflow-gpu to a CUDA .dll. Because of this I'm just going to run with CPU TensorFlow right now, since the main point of this project is not GPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "/cpu:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\fake_realdonaldtrump\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: No GPU found. Please use a GPU to train your nerual network.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "#Check tensorflow version\n",
    "assert LooseVersion(\"1.0\") <= LooseVersion(tf.__version__), \"Please use TensorFlow version 1.0 or newer\"\n",
    "print(\"TensorFlow Version: {}\".format(tf.__version__))\n",
    "\n",
    "#Check for GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn(\"No GPU found. Please use a GPU to train your nerual network.\")\n",
    "else:\n",
    "    print(\"Default GPU device: {}\".format(tf.test.gpu_device_name()))\n",
    "    \n",
    "#Output available devices\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "for d in devices:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Create TensorFlow placeholders for the neural network for the input text, targets, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Returns a size 3 tuple of TensorFlow placehodlers for the input text, targets,\n",
    "    and learning rate used by for the RNN.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        tf.placeholder(tf.int32, shape=[None, None], name=\"input\"),\n",
    "        tf.placeholder(tf.int32, shape=[None, None], name=\"targets\"),\n",
    "        tf.placeholder(tf.float32, name=\"learningrate\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell and RNN Size\n",
    "Stack one or more long-short term memory cells using TensorFlow's BasicLSTMCell and MultiRNNCell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Creates an RNN cell and initializes it.\n",
    "    :param batch_size: Size of input batches\n",
    "    :param rnn_size: Size of a MultiRNNCell\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm_size = 256\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCelll([lstm] * rnn_size)\n",
    "    initial_state = tf.identitiy(cell.zero_state(batch_size, tf.float32), name=\"initial_state\")\n",
    "    return (cell, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "Apply the word embedding to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Return embedding for input_data.\n",
    "    :param input_data: TensorFlow placeholder for text input\n",
    "    :param vocab_size: Number of words in vocabulary\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the RNN\n",
    "The single RNN cell has already been designed. Now design the recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create an RNN using the RNN cell\n",
    "    :param cell: RNN cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (outputs, final state)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, \"final_state\")\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulid the entire Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the NN\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of RNN\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (logits, final_state)\n",
    "    \"\"\"\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    ouputs, final_state = build_rnn(cell, embedding)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "Implement a batching function to create batches of input and targets. The batches should be a NumPy array with the shape (number of batches, 2, batch size, sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their IDs\n",
    "    :param batch_size: The size of the batch\n",
    "    :param seq_length: The length of the sequence\n",
    "    :return: Batches as a NumPy array\n",
    "    \"\"\"\n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    #Initialize batches\n",
    "    batches = [[[], []] for i in range(num_batches)]\n",
    "    #Truncate input\n",
    "    int_text = int_text[:batch_size*seq_length*num_batches]\n",
    "    \n",
    "    #Iterate until all elements are used\n",
    "    for i in range(0, len(int_text), seq_length*num_batches):\n",
    "        for j in range(num_batches):\n",
    "            start = i + seq_length * j\n",
    "            end = start + seq_length\n",
    "            curr_in = int_text[start:end]\n",
    "            curr_out = []\n",
    "            for k in range(seq_length):\n",
    "                curr_out.append(int_text[(start + k + 1) % len(int_text)])\n",
    "                batches[j][0].append(curr_in)\n",
    "                batches[j][1].append(curr_out)\n",
    "    return np.array(batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
